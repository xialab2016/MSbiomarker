---
title: "Manuscript_Code"
author: "Chenyi Chen"
date: "4/25/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# library 
```{r message=FALSE, warning=FALSE}
library(VennDiagram)
library(readr)
library(writexl) 
library(MASS) 
library(tidyr)
library(ggplot2)
library(pROC)
library(ROCR)
library(randomForest)
library(caTools)
library(caret)
library(glmnet)
library(corrplot)
library(plotmo) 
library(gplots) 
library(MLGL)
library(lmvar)
library(nnet)
library(grpreg)
library(gridExtra)
library(tableone)
library(dplyr)
library(xgboost)
library(psychometric)
library(e1071)
require(openxlsx)
```


# read data 
```{r read data, warning=FALSE}
setwd("D:/OneDrive/OneDrive - University of Pittsburgh/MS_XiaLab/Oactive/File")
RM<- read_csv("RMMSClog220128.csv", col_types = cols(...1 = col_skip()))
Pitt1 <- read_csv("Pittlog_batch1.csv", col_types = cols(...1 = col_skip()))
Pitt2 <- read_csv("Pittlog_batch2.csv", col_types = cols(...1 = col_skip()))
# Pitt2 <- 
total_all_pitt <- read_csv("total_all_pitt.csv", 
    col_types = cols(serum_date = col_skip()))

Pitt2$id = Pitt2$id_participant

Pitt2= Pitt2[,c(2:ncol(Pitt2))]
Pitt2[,c(1:21)] = log(Pitt2[,c(1:21)])
Pitt2$cohort = 5
pitt = bind_rows(Pitt1, Pitt2)
pitt$pitt = 1
RM$pitt = 0
pitt = subset(pitt, select = -c(col4a1_abs,gh_abs) )
RM = subset(RM, select = -c(col4a1_abs,gh_abs) )
names(pitt) <- gsub("_abs", "", names(pitt))
names(RM) <- gsub("_abs", "", names(RM))
RM = RM[complete.cases(RM),]
protein = toupper(c("aplp1", "ccl20", "cd6", "cdcp1", "cntn2", 
"cxcl13", "cxcl9", "flrt2", "gfap", 
"il12b", "mog", "nefl", "opg", "opn", "prtg", 
"serpina9", "tnfrsf10a", "tnfsf13b"
,"vcan"
))
names(pitt) <- c("id",protein,"Age", "pdds", "Sex", "dmt",
"Subtype" , "Disease_Duration", "PDDS_Time", 
"cohort", "pdds_cat", "Diagnose_Duration", "RaceEthnicity", "DMT_Efficacy", "pitt")

names(RM) <- c("id", "Age",  "Sex", "Disease_Duration", "pdds", toupper(c("opn", "cntn2", 
"serpina9", "prtg", "cxcl9", "cxcl13", "tnfsf13b", "cd6", "opg", 
"vcan", "tnfrsf10a", "il12b", "gfap", "mog", "cdcp1", "ccl20", 
"nefl", "flrt2", "aplp1")),"dmt", "PDDS_Time", "Subtype", 
"cohort", "pdds_cat", "Diagnose_Duration", "RaceEthnicity", "DMT_Efficacy", "pitt")
All_patients_none = bind_rows(pitt, RM)
All_patients_none = subset(All_patients_none, select = -c(Disease_Duration))

All_patients = 
  All_patients_none %>% filter(Diagnose_Duration<1000 & Subtype != 6) %>% left_join(total_all_pitt, by = c("id" = "id_participant") ) %>% rename( PROMIS= tscore, MSRSR = total, Disease_Duration = Diagnose_Duration, PROMIS_Time = PROMIS_duration)%>% mutate(pdds = ifelse(pdds == 7, 6, pdds),Subtype = ifelse(Subtype == 4 | Subtype == 5,1,0 ))

adjustVar = c( "Age", "Sex", "Subtype", 
"Disease_Duration", "RaceEthnicity", "DMT_Efficacy", "PDDS_Time")

# save(All_patients, file = "All_patients.RData")
# load("All_patients.RData")
```
# Patient Characteristics(Table 1)
```{r summary stats, warning= F}

remove = c("id",
           "dmt")
cor_df = All_patients[,(!names(All_patients) %in% remove)]
cor_df = cor_df %>% mutate(PROMIS_Bi  = ifelse(PROMIS<35,0,1) ) %>% dplyr::select(c(protein,"Age", "Sex", "Subtype", 
"Disease_Duration", "RaceEthnicity", "DMT_Efficacy", "PDDS_Time", 
"pdds", "pdds_cat", "PROMIS", "PROMIS_Bi", "PROMIS_Time","pitt")) 
var = c("Age", "Sex", "Subtype", 
"Disease_Duration", "RaceEthnicity", "DMT_Efficacy", "PDDS_Time", 
"pdds", "pdds_cat", "PROMIS", "PROMIS_Bi", "PROMIS_Time")
catVar = c( "pdds_cat","PROMIS_Bi", "Sex", "DMT_Efficacy", "Subtype", "RaceEthnicity")
# summary_tbl = CreateTableOne(vars = var, strata = "pitt" , data = cor_df, factorVars = catVar)
tab3Mat = print(summary_tbl, exact = "stage", smd = F)
setwd("D:/OneDrive/OneDrive - University of Pittsburgh/MS_XiaLab/Oactive/Code")
ID_PITT = All_patients %>% filter(pitt == 1) %>% select(id)

# write.csv(ID_PITT, file = "ID_PITT_Octave.csv")
# write.csv(tab3Mat, file = "StatsTable.csv")
```


# Feature Correlation(eFigure 2)
```{r correlation,warning=FALSE,fig.height=10.5, fig.width=9}

cor_df_plot = subset(cor_df, select = -c(PROMIS_Bi,pdds_cat,pitt) )

cor_df_plot = cor_df_plot %>% dplyr::select(c("APLP1", "CCL20", "CD6", "CDCP1", "CNTN2", "CXCL13", "CXCL9", 
"FLRT2", "GFAP", "IL12B", "MOG", "NEFL", "OPG", "OPN", "PRTG", 
"SERPINA9", "TNFRSF10A", "TNFSF13B", "VCAN", "Age", "Sex", "Subtype", 
"Disease_Duration", "RaceEthnicity", "DMT_Efficacy", "PDDS_Time","PROMIS_Time", 
"pdds", "PROMIS")) 
names(cor_df_plot) = c("APLP1", "CCL20", "CD6", "CDCP1", "CNTN2", "CXCL13", "CXCL9", 
"FLRT2", "GFAP", "IL12B", "MOG", "NEFL", "OPG", "OPN", "PRTG", 
"SERPINA9", "TNFRSF10A", "TNFSF13B", "VCAN", "Age", "Sex", "Subtype", 
"Disease Duration", "Race Ethnicity", "DMT Efficacy", "PDDS Time","PROMIS Time", 
"PDDS", "PROMIS")

M<-cor(cor_df_plot, use = "complete.obs",method = "spearman")
library(RColorBrewer)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)    
  p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
cor_allp = as.matrix(cor_df_plot)
p.mat <- cor.mtest(cor_allp)

svg("COR.svg",width = 10, height =11 )
corrplot(M, method="circle", col=col(200),
         number.cex= 17/ncol(M), #change the font size of number inside the cells
         type="lower",is.corr = FALSE,
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=90, #Text label color and rotation
         p.mat = p.mat, sig.level = 0.05, insig = "blank",
         diag=FALSE
         )
dev.off()

```
# Principal component analysis (eFigure 1)

```{r PCA plot}
pca.sample = All_patients[!duplicated(All_patients$id),]
pca.sample[,c(protein)]= scale(pca.sample[,c(protein)])
rownames(pca.sample) = pca.sample$id
pca.sample$pitt = as.factor(pca.sample$pitt)
pca.sample$cohort = as.factor(pca.sample$cohort)
pc <- prcomp(pca.sample[,c(protein)],
             center = T,
            scale. = F)
pc.var = pc$sdev^2
pca.var.perc = round(pc.var/sum(pc.var)*100,1)
library(ggfortify)
library(factoextra)
svg("PCA.svg",width = 7, height = 5.2)
autoplot(pc, data=pca.sample, colour="cohort",frame=TRUE) 
dev.off()


# Get the PCA results
pca_df <- data.frame(pc$x)

# Attach the group variable to the PCA results
pca_df$cohort = as.factor(pca.sample$cohort)

# Load the library
library(ggplot2)
svg(paste("PCA_new_connected",today(),".svg"),width = 7, height = 5.2)
pca_hull <- 
  pca_df %>% 
  group_by(cohort) %>% 
  slice(chull(PC1, PC2))

# Create the plot
ggplot(pca_df, aes(x = PC1, y = PC2, shape = cohort, color = cohort)) +
  geom_point() +
  theme_bw() +
  labs(x = "PC1 (31.3%)", y = "PC2 (10.0%)") +
  scale_shape_manual(values = c(16, 17, 25, 21, 22)) +
  # stat_ellipse() 
  geom_polygon(data = pca_hull,
               aes(fill = cohort,
                   colour = cohort),
               alpha = 0.3,
               show.legend = FALSE)

dev.off()

```


```{r}
pca.sample = All_patients[!duplicated(All_patients$id),]
pca.sample[,c(protein)]= scale(pca.sample[,c(protein)])
rownames(pca.sample) = pca.sample$id
pca.sample$pitt = as.factor(pca.sample$pitt)
pca.sample$cohort = as.factor(pca.sample$cohort)
pc <- prcomp(pca.sample[,c(protein)],
             center = T,
            scale. = F)



batch1 = All_patients[!duplicated(All_patients$id),] %>% filter(cohort == 1)
batch2 = All_patients[!duplicated(All_patients$id),] %>% filter(cohort == 2)
batch3 = All_patients[!duplicated(All_patients$id),] %>% filter(cohort == 3)
batch4 = All_patients[!duplicated(All_patients$id),] %>% filter(cohort == 4)
batch5 = All_patients[!duplicated(All_patients$id),] %>% filter(cohort == 5)

batches <- list(batch1, batch2, batch3, batch4, batch5)

pca_results <- lapply(batches, function(batch) {
  prcomp(batch[,c(protein)], center = TRUE, scale. = F)
})
df_batch <- data.frame(batch = integer(), PC1 = numeric(), PC2 = numeric())

for (i in 1:length(pca_results)) {
  n <- nrow(pca_results[[i]]$x)
  df_batch <- rbind(df_batch, data.frame(batch = rep(i, n),
                             PC1 = pca_results[[i]]$x[,1],
                             PC2 = pca_results[[i]]$x[,2]))
}

anova_pc1 <- aov(PC1 ~ as.factor(batch), data = df_batch)
anova_pc2 <- aov(PC2 ~ as.factor(batch), data = df_batch)

summary(anova_pc1)
summary(anova_pc2)
extract_anova_summary <- function(anova_obj) {
  summ <- summary(anova_obj)
  
  df <- data.frame(
    Term = rownames(summ[[1]]),
    Df = summ[[1]]$Df,
    F_value = summ[[1]]$`F value`,
    Pr = summ[[1]]$`Pr(>F)`
  )
  
  return(df)
}
df_pc1 <- extract_anova_summary(anova_pc1)
df_pc2 <- extract_anova_summary(anova_pc2)

write_xlsx(list(PC1 = df_pc1, PC2 = df_pc2), paste("anova_summary",today(),".xlsx"))
```

# ML Model Performance (Figure 2):
# LASSO regression model 
```{r build model for linear lasso }

eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))
  data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}

attach_name = function (protein,adjustVar)
{
  if (!is.null(protein) & !is.null(adjustVar)){
    dataset = c("Protein & Clinical")
  }
    
    else{
      if ( !is.null(protein) & is.null(adjustVar) ){
        dataset = c("Protein Only")
      }
    
  else {
    if ( is.null(protein) & !is.null(adjustVar) ){
        dataset = c("Clinical Only")
      }
        }
    }
}
 
Build_LASSO_model = function(seed,ratio,protein,adjustVar,outcome,pitt,RM){
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c(protein,adjustVar,outcome)]
  RMM = RM[complete.cases(RM),c(protein,adjustVar,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  }else
  {
    pitt =  All_patients[complete.cases(All_patients),c(protein,adjustVar,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
  }
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])
  lambdas <- 10^seq(2, -3, length = 500)
  cv_model <- cv.glmnet(x, y_train, alpha = 1,lambda = lambdas, standardize = TRUE,nfolds = 10)
  
  best_lambda <- cv_model$lambda.min
  
  best_model <- glmnet(x, y_train, alpha = 1, lambda = best_lambda)
  lasso.result.coef = as.data.frame(as.matrix(coef(best_model)))
  lasso.result.coef = lasso.result.coef[-1, , drop = F]
  lasso.result.coef = round(lasso.result.coef,6)
  predict(cv_model, s = "lambda.min", type = "coefficients")
  
  y_predicted_test <- predict(best_model, s = best_lambda, newx = x_test)
  eva = eval_results(y_test, y_predicted_test, test)
  ci = (as.numeric(CI.Rsq(eva$Rsquare,nrow(test),(ncol(test)-1))))
  
  r2ci = as.data.frame(c(paste(round(ci[1],2),"(",round(ci[3],2),",",round(ci[4],2),")")))
  feature_set = attach_name(protein,adjustVar)
  
  row.names(r2ci) = c(paste("R^2Ci",feature_set))
  names(r2ci) = c("Coefficient")
  names(lasso.result.coef) = c("Coefficient")
  
  Lasso_linear = rbind(r2ci,lasso.result.coef) 
  

}


```


## use continuous outcome with OLS LASSO regression 

### PDDS(eTable 4)

```{r}
seed = 2042
ratio =.8
protein_null = NULL
adjustVar_null = NULL
outcome = c("pdds")
LASSO_full_linear = Build_LASSO_model(seed,ratio,protein,adjustVar,outcome,pitt,RM)
LASSO_clnical_linear = Build_LASSO_model(seed,ratio,protein_null,adjustVar,outcome,pitt,RM)
LASSO_protein_linear = Build_LASSO_model(seed,ratio,protein,adjustVar_null,outcome,pitt,RM)

Lasso_linear_coef = rbind(LASSO_full_linear,LASSO_clnical_linear,LASSO_protein_linear)

```

### PROMIS(eTable 5)

```{r}
seed = 23
ratio =.8
protein_null = NULL
adjustVar_null = NULL
RM_null = NULL
outcome = c("PROMIS")
adjustVar.PROMIS = c( "Age", "Sex", "Subtype", 
"Disease_Duration", "RaceEthnicity", "DMT_Efficacy", "PROMIS_Time")

LASSO_full_linear = Build_LASSO_model(seed,ratio,protein,adjustVar.PROMIS,outcome,All_patients,RM_null)
LASSO_clnical_linear = Build_LASSO_model(seed,ratio,protein_null,adjustVar.PROMIS,outcome,All_patients,RM_null)
LASSO_protein_linear = Build_LASSO_model(seed,ratio,protein,adjustVar_null,outcome,All_patients,RM_null)

Lasso_linear_coef_PROMIS = rbind(LASSO_full_linear,LASSO_clnical_linear,LASSO_protein_linear)

```

## use binary outcome with LASSO regression 

model function: 
```{r}
LASSO.model =  function(seed,ratio,protein,adjustVar,outcome,pitt,RM,cutoff,run_comb){
# combined model --------------------------------
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c("id",protein,adjustVar,outcome)]
  RMM = RM[complete.cases(RM),c("id",protein,adjustVar,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  train_id = train$id
  test_id = test$id
  train = train[,-1]
  test = test[,-1]
  
  }else
  {
    pitt =  pitt[complete.cases(pitt),c("id",protein,adjustVar,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
    train_id = train$id
    test_id = test$id
    train = train[,-1]
    test = test[,-1]
  } 
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])
  
  lambdas <- 10^seq(2, -3, length = 500)
  cv_model <- cv.glmnet(x, y_train, alpha = 1,lambda = lambdas, standardize = TRUE,nfolds = 10, type = "auc")
  
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x, y_train, alpha = 1, lambda = best_lambda)
  lasso.result.coef = as.data.frame(as.matrix(coef(best_model)))
  lasso.result.coef = lasso.result.coef[-1, , drop = F]
  lasso.result.coef = round(lasso.result.coef,6)
    
  predictions_test_full <- predict(best_model, s = best_lambda, newx = x_test)
  
  new_pred_lasso = as.data.frame(ifelse(predictions_test_full >= cutoff, 1, 0))
  
  predictions_train_full <- predict(best_model, s = best_lambda, newx = x)

  data_lasso = cbind(test[,c(outcome)], new_pred_lasso)
  names(data_lasso) = c("actual", "pred")
  xtab_lasso = table(data_lasso$actual, data_lasso$pred)
  
  cm_lasso = confusionMatrix(xtab_lasso, mode = "everything", positive="1")
  F1Score = round(as.data.frame(cm_lasso$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")
  
  test$lasso.prob <- predict(best_model,type="response",newx = x_test, s = 'best_lambda')
  pred <- prediction(test$lasso.prob, test[, c(outcome)])
  auc_roc_comb = roc(y_test~ predictions_test_full)
  ci = as.numeric(ci.auc(auc_roc_comb))

  perf <- performance(pred,"tpr","fpr")
  auc11 = round(performance(pred,"auc")@y.values[[1]],2)
  auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
  names(auc1) = c("Coefficient")
  row.names(auc1) = c("CombinedCI")
  names(lasso.result.coef) = c("Coefficient")
  lasso.result.coef = rbind(auc1,F1Score,lasso.result.coef)
  predictions_test_full = data.frame(test_id,predictions_test_full)
  predictions_train_full = data.frame(train_id,predictions_train_full)

  if (run_comb == T){
# protein model --------------------------------
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c(protein,outcome)]
  RMM = RM[complete.cases(RM),c(protein,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  }else
  {
    pitt =  All_patients[complete.cases(All_patients),c(protein,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
  } 
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])
  
  lambdas <- 10^seq(2, -3, length = 500)
  cv_model <- cv.glmnet(x, y_train, alpha = 1,lambda = lambdas, standardize = TRUE,nfolds = 10, type = "auc")
  
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x, y_train, alpha = 1, lambda = best_lambda)
  lasso.result.coef3 = as.data.frame(as.matrix(coef(best_model)))
  lasso.result.coef3 = lasso.result.coef3[-1, , drop = F]
  lasso.result.coef3 = round(lasso.result.coef3,6)
    
  predictions_test <- predict(best_model, s = best_lambda, newx = x_test)
  
  new_pred_lasso = as.data.frame(ifelse(predictions_test >= cutoff, 1, 0))
  data_lasso = cbind(test[,c(outcome)], new_pred_lasso)
  names(data_lasso) = c("actual", "pred")
  xtab_lasso = table(data_lasso$actual, data_lasso$pred)
  
  cm_lasso = confusionMatrix(xtab_lasso, mode = "everything", positive="1")
  F1Score = round(as.data.frame(cm_lasso$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")
  
  test$lasso.prob <- predict(best_model,type="response",newx = x_test, s = 'best_lambda')
  pred <- prediction(test$lasso.prob, test[, c(outcome)])
  auc_roc_1 = roc(y_test~ predictions_test)
  ci = as.numeric(ci.auc(auc_roc_1))
  perf3 <- performance(pred,"tpr","fpr")
  auc33 =  round(performance(pred,"auc")@y.values[[1]],2)
  test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)
  auc3 = as.data.frame(c(ProteinCI = (paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")), p.value = round(test_result_1$p.value,2) ))
  names(auc3) = c("Coefficient")
  # row.names(auc3) = c("ProteinCI")
  names(lasso.result.coef3) = c("Coefficient")

      
  lasso.result.coef = rbind(lasso.result.coef,auc3,F1Score,lasso.result.coef3)
# Clinical model --------------------------------
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c(adjustVar,outcome)]
  RMM = RM[complete.cases(RM),c(adjustVar,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  }else
  {
    pitt =  All_patients[complete.cases(All_patients),c(adjustVar,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
  } 
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])
  
  lambdas <- 10^seq(2, -3, length = 500)
  cv_model <- cv.glmnet(x, y_train, alpha = 1,lambda = lambdas, standardize = TRUE,nfolds = 10, type = "auc")
  
  best_lambda <- cv_model$lambda.min
  best_model <- glmnet(x, y_train, alpha = 1, lambda = best_lambda)
  lasso.result.coef2 = as.data.frame(as.matrix(coef(best_model)))
  lasso.result.coef2 = lasso.result.coef2[-1, , drop = F]
  lasso.result.coef2 = round(lasso.result.coef2,6)
    
  predictions_test <- predict(best_model, s = best_lambda, newx = x_test)
  
  new_pred_lasso = as.data.frame(ifelse(predictions_test >= cutoff, 1, 0))
  

  data_lasso = cbind(test[,c(outcome)], new_pred_lasso)
  names(data_lasso) = c("actual", "pred")
  xtab_lasso = table(data_lasso$actual, data_lasso$pred)
  
  cm_lasso = confusionMatrix(xtab_lasso, mode = "everything", positive="1")
  F1Score = round(as.data.frame(cm_lasso$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")
  
  test$lasso.prob <- predict(best_model,type="response",newx = x_test, s = 'best_lambda')
  pred <- prediction(test$lasso.prob, test[, c(outcome)])
  auc_roc_2 = roc(y_test~ predictions_test)
  ci = as.numeric(ci.auc(auc_roc_2))
  perf2 <- performance(pred,"tpr","fpr")
  auc22 = round(performance(pred,"auc")@y.values[[1]],2)
  
    test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)
  auc2 = as.data.frame(c(ClinicalCI = (paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")), p.value.Clinical = round(test_result_2$p.value,2) ))
  names(auc2) = c("Coefficient")
  
  
  names(lasso.result.coef2) = c("Coefficient")
  lasso.result.coef = rbind(lasso.result.coef,auc2,F1Score,lasso.result.coef2)


    return(c(lasso.result.coef=lasso.result.coef, names = list(row.names(lasso.result.coef)), preds = as.data.frame(predictions_test_full), preds_train = as.data.frame(predictions_train_full), roc_protein = test_result_1$p.value, roc_clinical = test_result_2$p.value ))
    


  }
  
  
  else {
  return(lasso.result.coef)
  }

}

```

## PDDS(eTable 4)
```{r}
seed = 2042
ratio = 0.8
outcome = c("pdds_cat")
cutoff= 0.4
run_comb = T
LASSO =  LASSO.model(seed,ratio,protein,adjustVar,outcome,pitt,RM,cutoff,run_comb)
preds_lasso_test = data.frame(LASSO$preds.test_id,LASSO$preds.s1)
preds_lasso_train= data.frame(LASSO$preds_train.train_id,LASSO$preds_train.s1)
names(preds_lasso_test) = names(preds_lasso_train)
preds_lasso_PDDS = rbind(preds_lasso_test,preds_lasso_train)
Lasso_Binary_coef_PDDS = LASSO$lasso.result.coef.Coefficient

Lasso_Binary_coef_PDDS = data.frame(LASSO$names,Lasso_Binary_coef_PDDS)


```

## PROMIS(eTable 5)

```{r}
PROMIS_seed = 23
seed = PROMIS_seed
RM_null = NULL
All_patients = All_patients %>% mutate(PROMIS_Bi  = ifelse(PROMIS<35,0,1) )

ratio = 0.8
outcome = c("PROMIS_Bi")
cutoff= 0.6
run_comb = T
Lasso_Binary_coef_PROMIS = LASSO.model(seed,ratio,protein,adjustVar.PROMIS,outcome,All_patients,RM_null,cutoff,run_comb)
preds_lasso_promis_train = data.frame(Lasso_Binary_coef_PROMIS$preds_train.train_id, Lasso_Binary_coef_PROMIS$preds_train.s1)
preds_lasso_promis =  data.frame(Lasso_Binary_coef_PROMIS$preds.test_id,Lasso_Binary_coef_PROMIS$preds.s1) 
names(preds_lasso_promis) = names(preds_lasso_promis_train)
preds_lasso_PROMIS = rbind(preds_lasso_promis,preds_lasso_promis_train)

Lasso_Binary_coef_PROMIS = data.frame(Lasso_Binary_coef_PROMIS$names,Lasso_Binary_coef_PROMIS$lasso.result.coef.Coefficient)

ci1 = as.numeric(substr(Lasso_Binary_coef_PROMIS[Lasso_Binary_coef_PROMIS$Lasso_Binary_coef_PROMIS.names == 'CombinedCI',2],1,4))
ci2 = as.numeric(substr(Lasso_Binary_coef_PROMIS[Lasso_Binary_coef_PROMIS$Lasso_Binary_coef_PROMIS.names == 'ProteinCI',2],1,4))
ci3 = as.numeric(substr(Lasso_Binary_coef_PROMIS[Lasso_Binary_coef_PROMIS$Lasso_Binary_coef_PROMIS.names == 'ClinicalCI',2],1,4))
p.value.Clinical = as.numeric(Lasso_Binary_coef_PROMIS[Lasso_Binary_coef_PROMIS$Lasso_Binary_coef_PROMIS.names == "p.value.Clinical",2])
PROMIS_seed = seed



```

# Alternative machine learning models (eTable 6, eTable 7):
# Support Vector Machine (SVM)


```{r build svm mode, warning= F}

svm_model = function(seed,ratio,pitt,RM,protein,adjustVar,outcome,cutoff){
  
  
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c("id",protein,adjustVar,outcome)]
  RMM = RM[complete.cases(RM),c("id",protein,adjustVar,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  train_id = train$id
  test_id = test$id
  train = train[,-1]
  test = test[,-1]
  }else
  {
    pitt =  All_patients[complete.cases(All_patients),c("id",protein,adjustVar,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
    train_id = train$id
    test_id = test$id
    train = train[,-1]
    test = test[,-1]
  } 
  pitt = rbind(train,test)
  if (outcome == "pdds_cat"){
  pitt$pdds_cat = as.factor(pitt$pdds_cat)
  }  else {
  pitt$PROMIS_Bi= as.factor(pitt$PROMIS_Bi)
  }
  all_x <- data.matrix(pitt[,!(names(pitt) %in% outcome)])
  all_y = as.numeric(unlist(pitt[,(names(pitt) %in% outcome)]))
  Imp.svm <- rfe(all_x,all_y,
             sizes = c(5,
                         10,15,20,21,22,23,24,25,26),
                       
             rfeControl = rfeControl(functions = caretFuncs,
                                     verbose = FALSE,number = 5),
                               method = "svmRadial")
  feature = as.data.frame(predictors(Imp.svm))
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])
  model_svm = svm(y_train ~ .,
                 data = train,
                 cost = 10,scale = T,
                 kernel = 'radial')
  set.seed(seed)
  if (outcome == "pdds_cat"){
    tmodel=tune(svm,pdds_cat ~., data=train,
  ranges=list(cost = c(0.001, 0.01, 0.1, 1,5,10,15)))
  best_model = tmodel$best.model
  }  else {
    tmodel=tune(svm,PROMIS_Bi ~., data=train,
  ranges=list(cost = c(0.001, 0.01, 0.1, 1,5,10,15)))
  best_model = tmodel$best.model
  }
  
  pred <- predict(best_model, x_test, type="prob")
  new_pred_svm = as.data.frame(ifelse(pred >= cutoff, 1, 0))  
  
  pred_train <- predict(best_model, x, type="prob")
  new_pred_svm_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))
  
  data_svm = cbind(test[,c(outcome)], new_pred_svm)
  names(data_svm) = c("actual", "pred")
  xtab_svm = table(data_svm$actual, data_svm$pred)
    
  cm_svm = confusionMatrix(xtab_svm, mode = "everything", positive="1")
  F1Score = round(as.data.frame(cm_svm$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")

  # Model Performance Statistics
  pred_val <-prediction(pred, y_test)
  auc = roc(y_test~ pred)
  
  ci = as.numeric(ci.auc(auc))
  auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
  names(auc1) = c("Coefficient")
  row.names(auc1) = c("Combined")

  # Calculating Area under Curve
  perf_val <- performance(pred_val,"auc")
  perf_val
  # Calculating True Positive and False Positive Rate
  perf_1 <- performance(pred_val, "tpr", "fpr")
  auc11 = round(perf_val@y.values[[1]],2)
  # Plot the ROC curve
  plot(perf_1, col = "green", lwd = 1.5)
  svm_coef = rbind(F1Score,auc1)
  pred_train = data.frame(train_id,pred_train)
  pred = data.frame(test_id,pred)
  return (list(performance = svm_coef, coefficient = feature, perf = perf_1, confusionMat = cm_svm, preds = as.data.frame(pred), preds_train = as.data.frame(pred_train), auc = auc))
}
  
```

## PDDS 
```{r run PDDS svm model,warning = F, message = F}
outcome = c("pdds_cat")
seed = 2042
ratio = 0.8
cutoff = 0.25
SVM_PDDS_combine = svm_model(seed,ratio,pitt,RM,protein,adjustVar,outcome,cutoff)
auc11 = as.numeric(substr(SVM_PDDS_combine$performance[7,],1,4))
perf = SVM_PDDS_combine$perf
SVM_PDDS_combine_perf = as.data.frame((SVM_PDDS_combine$performance))
row.names(SVM_PDDS_combine_perf) = c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI")
names(SVM_PDDS_combine_perf) = c("Coef")
SVM_PDDS_Feature = SVM_PDDS_combine$coefficient

pred_SVM = data.frame(SVM_PDDS_combine$preds.test_id,SVM_PDDS_combine$preds.pred)
pred_SVM_train = data.frame(SVM_PDDS_combine$preds_train.train_id,SVM_PDDS_combine$preds_train.pred_train)
names(pred_SVM_train) = names(pred_SVM)
pred_SVM_PDDS = rbind(pred_SVM_train,pred_SVM)
auc_roc_comb = SVM_PDDS_combine$auc

SVM_PDDS_protein = svm_model(seed,ratio,pitt,RM,protein,adjustVar_null,outcome,cutoff)
auc_roc_1 = SVM_PDDS_protein$auc
test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)
auc33 = as.numeric(substr(SVM_PDDS_protein$performance[7,],1,4))
perf3 = SVM_PDDS_protein$perf
SVM_PDDS_protein_perf = as.data.frame(rbind(SVM_PDDS_protein$performance,round(test_result_1$p.value,2)))
row.names(SVM_PDDS_protein_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_protein","p.value.Protein")
names(SVM_PDDS_protein_perf) = c("Coef")

      
SVM_PDDS_clinical = svm_model(seed,ratio,pitt,RM,protein_null,adjustVar,outcome,cutoff)
auc_roc_2 = SVM_PDDS_clinical$auc
test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)
perf2 = SVM_PDDS_clinical$perf
auc22 = as.numeric(substr(SVM_PDDS_clinical$performance[7,],1,4))
SVM_PDDS_clinical_perf = as.data.frame(rbind(SVM_PDDS_clinical$performance,round(test_result_2$p.value,2)))
row.names(SVM_PDDS_clinical_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_Clinical","p.value.Clinical")
names(SVM_PDDS_clinical_perf) = c("Coef")

SVM_PDDS_total = rbind(SVM_PDDS_combine_perf,SVM_PDDS_protein_perf,SVM_PDDS_clinical_perf)
```

## PROMIS
```{r run PROMIS svm model,warning = F, message = F}
outcome = c("PROMIS_Bi")
seed = PROMIS_seed
ratio = 0.8
cutoff = 0.75

SVM_PROMIS_combine = svm_model(seed,ratio,All_patients,RM_null,protein,adjustVar.PROMIS,outcome,cutoff)
auc_roc_comb = SVM_PROMIS_combine$auc
auc11 = as.numeric(substr(SVM_PROMIS_combine$performance[7,],1,4))
perf = SVM_PROMIS_combine$perf
pred_SVM_PROMIS = data.frame(SVM_PROMIS_combine$preds.test_id,SVM_PROMIS_combine$preds.pred)
pred_SVM_PROMIS_train = data.frame(SVM_PROMIS_combine$preds_train.train_id,SVM_PROMIS_combine$preds_train.pred_train)
names(pred_SVM_PROMIS) =  names(pred_SVM_PROMIS_train)
pred_SVM_PROMIS = rbind(pred_SVM_PROMIS,pred_SVM_PROMIS_train)

SVM_PROMIS_combine_perf = as.data.frame((SVM_PROMIS_combine$performance))
row.names(SVM_PROMIS_combine_perf) = c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI")
names(SVM_PROMIS_combine_perf) = c("Coef")
SVM_PROMIS_Feature = SVM_PROMIS_combine$`coefficient.predictors(Imp.svm)`
SVM_PROMIS_combine$confusionMat.table
cutoff = 0.78
SVM_PROMIS_protein = svm_model(seed,ratio,All_patients,RM_null,protein,adjustVar_null,outcome,cutoff)
auc_roc_1 = SVM_PROMIS_protein$auc
test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)


auc33 = as.numeric(substr(SVM_PROMIS_protein$performance[7,],1,4))
perf3 = SVM_PROMIS_protein$perf
SVM_PROMIS_protein_perf = as.data.frame(rbind(SVM_PROMIS_protein$performance,round(test_result_1$p.value,2)))
row.names(SVM_PROMIS_protein_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_protein","p.value.Protein")
names(SVM_PROMIS_protein_perf) = c("Coef")
SVM_PROMIS_protein$confusionMat.table

SVM_PROMIS_clinical = svm_model(seed,ratio,All_patients,RM_null,protein_null,adjustVar.PROMIS,outcome,cutoff)
auc_roc_2 = SVM_PROMIS_clinical$auc
test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)
perf2 = SVM_PROMIS_clinical$perf
auc22 = as.numeric(substr(SVM_PROMIS_clinical$performance[7,],1,4))
SVM_PROMIS_clinical_perf = as.data.frame(rbind(SVM_PROMIS_clinical$performance,round(test_result_2$p.value,2)))
row.names(SVM_PROMIS_clinical_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_Clinical","p.value.Clinical")
names(SVM_PROMIS_clinical_perf) = c("Coef")

SVM_PROMIS_total = rbind(SVM_PROMIS_combine_perf,SVM_PROMIS_protein_perf,SVM_PROMIS_clinical_perf)
```


# XGBoost

```{r}

xgb_model = function(seed,ratio,pitt,RM,protein,adjustVar,outcome,cutoff){
  
  
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c("id",protein,adjustVar,outcome)]
  RMM = RM[complete.cases(RM),c("id",protein,adjustVar,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  train_id = train$id
  test_id = test$id
  train = train[,-1]
  test = test[,-1]
  }else
  {
    pitt =  All_patients[complete.cases(All_patients),c("id",protein,adjustVar,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
    train_id = train$id
    test_id = test$id
    train = train[,-1]
    test = test[,-1]
  } 
  pitt = rbind(train,test)
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])
  xgb.train.data <- xgb.DMatrix(data = x, label = y_train,missing = NA)
  param <- list(objective = "binary:logistic", base_score = 0.5)
  all_x <- data.matrix(pitt[,!(names(pitt) %in% outcome)])
  all_y = as.numeric(unlist(pitt[,(names(pitt) %in% outcome)]))
  xgb.train.data2 <- xgb.DMatrix(data = all_x, label = all_y,missing = NA)

  cv <- xgb.cv(data = xgb.train.data2,  nrounds = 1500, nthread = 20, nfold = 5, metrics = list("auc"),max_depth = 20,early_stopping_rounds = 100, eta = 1, objective = "binary:logistic")
  bestIte = cv$best_iteration
  
  best_model <- xgboost(param =param,  data = xgb.train.data, nrounds=bestIte)
  pred_train <- predict(best_model, x, type="prob")
  new_pred_xgb_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))
  pred <- predict(best_model, x_test, type="prob")
  new_pred_xgb = as.data.frame(ifelse(pred >= cutoff, 1, 0))
  data_xgb = cbind(test[,c(outcome)], new_pred_xgb)
  names(data_xgb) = c("actual", "pred")
  xtab_xgb = table(data_xgb$actual, data_xgb$pred)
    
  cm_xgb = confusionMatrix(xtab_xgb, mode = "everything", positive="1")
  F1Score = round(as.data.frame(cm_xgb$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")

  # Model Performance Statistics
  pred_val <-prediction(pred, y_test)
  auc = roc(y_test~ pred)
  ci = as.numeric(ci.auc(auc))
  auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
  names(auc1) = c("Coefficient")
  row.names(auc1) = c("Combined")

  # Calculating Area under Curve
  perf_val <- performance(pred_val,"auc")
  perf_val
  # Calculating True Positive and False Positive Rate
  perf_1 <- performance(pred_val, "tpr", "fpr")
  auc11 = round(perf_val@y.values[[1]],2)
  # Plot the ROC curve
  plot(perf_1, col = "green", lwd = 1.5)
  xgb_coef = rbind(F1Score,auc1)
  feature = xgb.importance(model = best_model)
  pred_train = data.frame(train_id,pred_train)
  pred = data.frame(test_id,pred)
  
  return (list(performance = xgb_coef, feature = feature, perf = perf_1, confusionMat = cm_xgb, preds= as.data.frame(pred),preds_train = as.data.frame(pred_train),auc = auc ))

}
```

## PDDS
```{r}
seed=2042
outcome = c("pdds_cat")
cutoff = 0.4
ratio = .8
adjustVar_null = NULL
protein_null = NULL

xgb_PDDS_combine = xgb_model(seed,ratio,pitt,RM,protein,adjustVar,outcome,cutoff)
auc_roc_comb = xgb_PDDS_combine$auc

preds_xgb = data.frame(xgb_PDDS_combine$preds)
preds_xgb_train = data.frame(xgb_PDDS_combine$preds_train)
names(preds_xgb_train) = names(preds_xgb)
preds_XGB_PDDS = rbind(preds_xgb_train,preds_xgb)

auc11 = as.numeric(substr(xgb_PDDS_combine$performance[7,],1,4))
perf = xgb_PDDS_combine$perf
xgb_PDDS_combine_perf = as.data.frame(rbind(xgb_PDDS_combine$performance))
row.names(xgb_PDDS_combine_perf) = c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI")
names(xgb_PDDS_combine_perf) = c("Coef")
xgb_PDDS_Feature = xgb_PDDS_combine$feature

xgb_PDDS_protein = xgb_model(seed,ratio,pitt,RM,protein,adjustVar_null,outcome,cutoff)
auc_roc_1 = xgb_PDDS_protein$auc
test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)

auc33 = as.numeric(substr(xgb_PDDS_protein$performance[7,],1,4))
perf3 = xgb_PDDS_protein$perf
xgb_PDDS_protein_perf = as.data.frame(rbind(xgb_PDDS_protein$performance,round(test_result_1$p.value,2)))
row.names(xgb_PDDS_protein_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_protein","p.value.Protein")
names(xgb_PDDS_protein_perf) = c("Coef")
  
xgb_PDDS_clinical = xgb_model(seed,ratio,pitt,RM,protein_null,adjustVar,outcome,cutoff)
auc_roc_2 = xgb_PDDS_clinical$auc
test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)

perf2 = xgb_PDDS_clinical$perf
auc22 = as.numeric(substr(xgb_PDDS_clinical$performance[7,],1,4))
xgb_PDDS_clinical_perf = as.data.frame(rbind(xgb_PDDS_clinical$performance,round(test_result_2$p.value,2)))
row.names(xgb_PDDS_clinical_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_Clinical","p.value.Clinical")
names(xgb_PDDS_clinical_perf) = c("Coef")

xgb_PDDS_total = rbind(xgb_PDDS_combine_perf,xgb_PDDS_protein_perf,xgb_PDDS_clinical_perf)
```
## PROMIS
```{r}
seed = PROMIS_seed
outcome = c("PROMIS_Bi")
cutoff = 0.55
ratio = .8

xgb_PROMIS_combine = xgb_model(seed,ratio,All_patients,RM_null,protein,adjustVar.PROMIS,outcome,cutoff)
auc_roc_comb = xgb_PROMIS_combine$auc
auc11 = as.numeric(substr(xgb_PROMIS_combine$performance[7,],1,4))
perf = xgb_PROMIS_combine$perf
xgb_PROMIS_combine_perf = as.data.frame((xgb_PROMIS_combine$performance))
row.names(xgb_PROMIS_combine_perf) = c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI")
names(xgb_PROMIS_combine_perf) = c("Coef")
xgb_PROMIS_Feature = xgb_PROMIS_combine$feature

preds_xgb_PROMIS = data.frame(xgb_PROMIS_combine$preds)

preds_xgb__PROMIS_train = data.frame(xgb_PROMIS_combine$preds_train)
names(preds_xgb__PROMIS_train) = names(preds_xgb_PROMIS)
preds_xgb_PROMIS = rbind(preds_xgb__PROMIS_train,preds_xgb_PROMIS)

xgb_PROMIS_protein = xgb_model(seed,ratio,All_patients,RM_null,protein,adjustVar_null,outcome,cutoff)
auc_roc_1 = xgb_PROMIS_protein$auc
test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)
auc33 = as.numeric(substr(xgb_PROMIS_protein$performance[7,],1,4))
perf3 = xgb_PROMIS_protein$perf
xgb_PROMIS_protein_perf = as.data.frame(rbind(xgb_PROMIS_protein$performance,round(test_result_1$p.value,2)))
row.names(xgb_PROMIS_protein_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_protein","p.value.Protein")
names(xgb_PROMIS_protein_perf) = c("Coef")
  
xgb_PROMIS_clinical = xgb_model(seed,ratio,All_patients,RM_null,protein_null,adjustVar.PROMIS,outcome,cutoff)
auc_roc_2 = auc_roc_1 = xgb_PROMIS_clinical$auc
test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)

# print(test_result_1)
# print(test_result_2)
perf2 = xgb_PROMIS_clinical$perf
auc22 = as.numeric(substr(xgb_PROMIS_clinical$performance[7,],1,4))
xgb_PROMIS_clinical_perf = as.data.frame(rbind(xgb_PROMIS_clinical$performance,round(test_result_2$p.value,2)))
row.names(xgb_PROMIS_clinical_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_Clinical","p.value.Clinical")
names(xgb_PROMIS_clinical_perf) = c("Coef")

xgb_PROMIS_total = rbind(xgb_PROMIS_combine_perf,xgb_PROMIS_protein_perf,xgb_PROMIS_clinical_perf)
    
```
# Random Forest (RF)
```{r}

rf_model = function(seed,ratio,pitt,RM,protein,adjustVar,outcome,cutoff){
  
  
  set.seed(seed)
  if (!is.null(RM)){
  PITT =  pitt[complete.cases(pitt),c("id",protein,adjustVar,outcome)]
  RMM = RM[complete.cases(RM),c("id",protein,adjustVar,outcome)]
  
  p = (sample(nrow(PITT), nrow(PITT)*ratio))
  Rm = (sample(nrow(RMM), nrow(RMM)*ratio))
  
  p1 = PITT[p,]
  rm1 = RMM[Rm,]
  
  p2 = PITT[-p,]
  rm2 = RMM[-Rm,]
  
  train <-rbind(p1,rm1)
  test <-rbind(p2,rm2)
  train_id = train$id
  test_id = test$id
  train = train[,-1]
  test = test[,-1]
  }else
  {
    pitt =  All_patients[complete.cases(All_patients),c("id",protein,adjustVar,outcome)]
    dt = (sample(nrow(pitt), nrow(pitt)*ratio))
    train<-pitt[dt,]
    test<-pitt[-dt,]
    train_id = train$id
    test_id = test$id
    train = train[,-1]
    test = test[,-1]
  } 
  
  y_train = as.numeric(unlist(train[,(names(train) %in% outcome)]))
  x <- data.matrix(train[,!(names(train) %in% outcome)])
  y_test = as.numeric(unlist(test[,(names(test) %in% outcome)]))
  x_test <- data.matrix(test[,!(names(test) %in% outcome)])

  if (outcome == "pdds_cat"){
    train$pdds_cat = as.factor(train$pdds_cat)
    rf <- randomForest( pdds_cat ~ .,  data=train,ntree = 1000)
  } else{
    train$PROMIS_Bi = as.factor(train$PROMIS_Bi)
    rf <- randomForest( PROMIS_Bi ~ .,  data=train,ntree = 1000)

  }

 
  pred <- predict(rf, x_test, type="prob")[,2]
  new_pred_rf = as.data.frame(ifelse(pred >= cutoff, 1, 0))
  pred_train <- predict(rf, x, type="prob")[,2]
  new_pred_rf_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))
  data_rf = cbind(test[,c(outcome)], new_pred_rf)
  names(data_rf) = c("actual", "pred")
  xtab_rf = table(data_rf$actual, data_rf$pred)
    
  cm_rf = confusionMatrix(xtab_rf, mode = "everything", positive="1")
  F1Score = round(as.data.frame(cm_rf$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")

  # Model Performance Statistics
  pred_val <-prediction(pred, y_test)
  auc = roc(as.factor(y_test)~ pred)
  ci = as.numeric(ci.auc(auc))
  auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
  names(auc1) = c("Coefficient")
  row.names(auc1) = c("Combined")

  # Calculating Area under Curve
  perf_val <- performance(pred_val,"auc")
  perf_val
  # Calculating True Positive and False Positive Rate
  perf_1 <- performance(pred_val, "tpr", "fpr")
  auc11 = round(perf_val@y.values[[1]],2)
  # Plot the ROC curve
  plot(perf_1, col = "green", lwd = 1.5)
  rf_coef = rbind(F1Score,auc1)
  feature = as.data.frame(importance(rf))
  feature$names <- row.names(feature)
  fea_names = attach_name(protein,adjustVar)
  svg(paste0("rf_",outcome,"_",fea_names,seed,".svg"),width = 4, height = 6.5 )
  varImpPlot(rf)
  dev.off()
  pred_train = data.frame(train_id,pred_train)
  pred = data.frame(test_id,pred)
  

  return (list(performance = rf_coef, feature = feature, perf = perf_1, confusionMat = cm_rf, preds = as.data.frame(pred), preds_train = as.data.frame(pred_train), auc = auc))

}
```

## PDDS
```{r}
seed=2042
outcome = c("pdds_cat")
cutoff = 0.33
ratio = .8
rf_PDDS_combine = rf_model(seed,ratio,pitt,RM,protein,adjustVar,outcome,cutoff)
auc_roc_comb = rf_PDDS_combine$auc
preds_rf = data.frame(rf_PDDS_combine$preds)
preds_rf_train = data.frame(rf_PDDS_combine$preds_train)
names(preds_rf_train) = names(preds_rf)
preds_rf_PDDS = rbind(preds_rf_train,preds_rf)
```
 
 
```{r}


auc11 = as.numeric(substr(rf_PDDS_combine$performance[7,],1,4))
perf = rf_PDDS_combine$perf
rf_PDDS_combine_perf = as.data.frame((rf_PDDS_combine$performance))
row.names(rf_PDDS_combine_perf) = c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI")
names(rf_PDDS_combine_perf) = c("Coef")
rf_PDDS_Feature = as.data.frame(rf_PDDS_combine$feature)
rf_PDDS_Feature <- rf_PDDS_Feature[order(-rf_PDDS_Feature$MeanDecreaseGini),]

rf_PDDS_protein = rf_model(seed,ratio,pitt,RM,protein,adjustVar_null,outcome,cutoff)
auc_roc_1 = rf_PDDS_protein$auc
test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)

auc33 = as.numeric(substr(rf_PDDS_protein$performance[7,],1,4))
perf3 = rf_PDDS_protein$perf
rf_PDDS_protein_perf = as.data.frame(rbind(rf_PDDS_protein$performance,round(test_result_1$p.value,2)))
row.names(rf_PDDS_protein_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_protein","p.value.Protein")
names(rf_PDDS_protein_perf) = c("Coef")
  
rf_PDDS_clinical = rf_model(seed,ratio,pitt,RM,protein_null,adjustVar,outcome,cutoff)
auc_roc_2 = rf_PDDS_clinical$auc
test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)

perf2 = rf_PDDS_clinical$perf
auc22 = as.numeric(substr(rf_PDDS_clinical$performance[7,],1,4))
rf_PDDS_clinical_perf = as.data.frame(rbind(rf_PDDS_clinical$performance,round(test_result_2$p.value,2)))
row.names(rf_PDDS_clinical_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_Clinical","p.value.Clinical")
names(rf_PDDS_clinical_perf) = c("Coef")

rf_PDDS_total = rbind(rf_PDDS_combine_perf,rf_PDDS_protein_perf,rf_PDDS_clinical_perf)
    
```

## PROMIS_BI
```{r}
seed=PROMIS_seed
outcome = c("PROMIS_Bi")
cutoff = 0.6
ratio = .8
rf_PROMIS_combine = rf_model(seed,ratio,All_patients,RM_null,protein,adjustVar.PROMIS,outcome,cutoff)
auc_roc_comb = rf_PROMIS_combine$auc

preds_rf_PROMIS = data.frame(rf_PROMIS_combine$preds)
preds_rf_PROMIS_train = data.frame(rf_PROMIS_combine$preds_train)
names(preds_rf_PROMIS_train) = names(preds_rf_PROMIS)
preds_rf_PROMIS = rbind(preds_rf_PROMIS_train,preds_rf_PROMIS)



auc11 = as.numeric(substr(rf_PROMIS_combine$performance[7,],1,4))
perf = rf_PROMIS_combine$perf
rf_PROMIS_combine_perf = as.data.frame((rf_PROMIS_combine$performance))
row.names(rf_PROMIS_combine_perf) = c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI")
names(rf_PROMIS_combine_perf) = c("Coef")

  
rf_PROMIS_Feature = as.data.frame(rf_PROMIS_combine$feature)
rf_PROMIS_Feature <- rf_PROMIS_Feature[order(-rf_PROMIS_Feature$MeanDecreaseGini),]

rf_PROMIS_protein = rf_model(seed,ratio,All_patients,RM_null,protein,adjustVar_null,outcome,cutoff)
auc_roc_1 = rf_PROMIS_protein$auc
test_result_1 <- roc.test(auc_roc_comb, auc_roc_1, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.90)

auc33 = as.numeric(substr(rf_PROMIS_protein$performance[7,],1,4))
perf3 = rf_PROMIS_protein$perf
rf_PROMIS_protein_perf = as.data.frame(rbind(rf_PROMIS_protein$performance,round(test_result_1$p.value,2)))
row.names(rf_PROMIS_protein_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_protein","p.value.Protein")
names(rf_PROMIS_protein_perf) = c("Coef")
  
rf_PROMIS_clinical = rf_model(seed,ratio,All_patients,RM_null,protein_null,adjustVar.PROMIS,outcome,cutoff)

auc_roc_2 = rf_PROMIS_clinical$auc
test_result_2 <- roc.test(auc_roc_comb, auc_roc_2, method = "bootstrap", alternative = "greater", boot.n=1000,conf.level = 0.95)

perf2 = rf_PROMIS_clinical$perf
auc22 = as.numeric(substr(rf_PROMIS_clinical$performance[7,],1,4))
rf_PROMIS_clinical_perf = as.data.frame(rbind(rf_PROMIS_clinical$performance,round(test_result_2$p.value,2)))
row.names(rf_PROMIS_clinical_perf) =c("Sensitivity","Specificity", "Pos Pred Value","Neg Pred Value","Precision","F1","CI_Clinical","p.value.Clinical")
names(rf_PROMIS_clinical_perf) = c("Coef")

rf_PROMIS_total = rbind(rf_PROMIS_combine_perf,rf_PROMIS_protein_perf,rf_PROMIS_clinical_perf)
    
```


```{r}
list_of_datasets <- list("LASSO PDDS" = Lasso_Binary_coef_PDDS,
                         "LASSO PROMIS" = Lasso_Binary_coef_PROMIS,
                         "SVM PDDS" = SVM_PDDS_total,
                         "SVM PROMIS" = SVM_PROMIS_total,
                         "XGB PDDS" = xgb_PDDS_total,
                         "XGB PROMIS" = xgb_PROMIS_total,
                         "RF PDDS" = rf_PDDS_total,
                         "RF PROMIS " = rf_PROMIS_total
                         )
write.xlsx(list_of_datasets, file = paste("ML_model_Diagnostics",PROMIS_seed,".xlsx"),rowNames = T,overwrite =T)

```

# PDDS: ensemble machine learning 

```{r}
set.seed(2042)
outcome = c("pdds_cat")
ratio = 0.8

PITT =  pitt[complete.cases(pitt),c("id", outcome)]
RMM = RM[complete.cases(RM),c("id", outcome)]

p = (sample(nrow(PITT), nrow(PITT)*ratio))
Rm = (sample(nrow(RMM), nrow(RMM)*ratio))

p1 = PITT[p,]
rm1 = RMM[Rm,]

p2 = PITT[-p,]
rm2 = RMM[-Rm,]

train <-rbind(p1,rm1)
test <-rbind(p2,rm2)
ensem_df = preds_XGB_PDDS %>% left_join(preds_lasso_PDDS, by = c("xgb_PDDS_combine.preds.test_id" = "LASSO.preds_train.train_id")) %>% 
left_join(pred_SVM_PDDS,by = c("xgb_PDDS_combine.preds.test_id" = "SVM_PDDS_combine.preds.test_id"))  %>% 
  left_join(preds_rf_PDDS, by = c("xgb_PDDS_combine.preds.test_id" = "rf_PDDS_combine.preds.test_id" )) %>% rename(id =
xgb_PDDS_combine.preds.test_id)
Ensem_train = train %>% left_join(ensem_df )
Ensem_test = test %>% left_join(ensem_df)
Ensem_train$pdds_cat = as.factor(train$pdds_cat)
Ensem_test$pdds_cat =as.factor(test$pdds_cat)
Ensem_train = Ensem_train[,-1]
Ensem_test = Ensem_test[,-1]
ensemble_names = c("xgb","lasso","svm","rf")
names(Ensem_train) = c(outcome,ensemble_names)
names(Ensem_test) = c(outcome,ensemble_names)

y_train = as.numeric(as.character(unlist(Ensem_train[,(names(Ensem_train) %in% outcome)])))
x <- data.matrix(Ensem_train[,!(names(Ensem_train) %in% outcome)])
y_test = as.numeric(as.character(unlist(Ensem_test[,(names(Ensem_test) %in% outcome)])))
x_test <- data.matrix(Ensem_test[,!(names(Ensem_test) %in% outcome)])
```

## RF

```{r}
rf <- randomForest( pdds_cat ~ .,  data=Ensem_train,ntree = 1000)
    
cutoff = 0.33
ratio = .8
  pred <- predict(rf, x_test, type="prob")[,2]
  new_pred_rf = as.data.frame(ifelse(pred >= cutoff, 1, 0))
  pred_train <- predict(rf, x, type="prob")[,2]
  new_pred_rf_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))
  data_rf = cbind(test[,c(outcome)], new_pred_rf)
  names(data_rf) = c("actual", "pred")
  xtab_rf = table(data_rf$actual, data_rf$pred)
    
  cm_rf = confusionMatrix(xtab_rf, mode = "everything", positive="1")
  cm_rf
  F1Score = round(as.data.frame(cm_rf$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")
  # Model Performance Statistics
  pred_val <-prediction(pred, y_test)
  auc = roc(as.factor(y_test)~ pred)
  ci = as.numeric(ci.auc(auc))
  auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
  names(auc1) = c("Coefficient")
  row.names(auc1) = c("Combined")
  F1Score[nrow(F1Score)+1,] = auc1
  # Calculating Area under Curve
  perf_val <- performance(pred_val,"auc")
  # Calculating True Positive and False Positive Rate
  perf_1 <- performance(pred_val, "tpr", "fpr")
  auc11 = round(perf_val@y.values[[1]],2)

```

```{r}

# Create the model
rf_model <- randomForest(pdds_cat ~ ., data = Ensem_train_rf, ntree = 1000, mtry = 2)

# Set up the tuning grid
tune_grid <- expand.grid(
  # ntree = c(500, 1000, 1500),
  mtry = c(2, 8, 4)
)

# Tune the model
tune_result <- train(
  pdds_cat ~ ., data = Ensem_train_rf,
  method = "rf",
  tuneGrid = tune_grid,ntree = 1000,
  trControl = trainControl(method = "cv", number = 5,classProbs = TRUE),
  metric = "Accuracy"
)

# Get the best model
best_model <- tune_result$bestTune
rf <- randomForest(pdds_cat ~ ., data = Ensem_train_rf,  mtry = best_model$mtry)

# ?train()
```


```{r}
Ensem_train_rf = Ensem_train %>% mutate(pdds_cat = ifelse (pdds_cat== 1,"Y","N")) 
Ensem_test_rf = Ensem_test %>% mutate(pdds_cat = ifelse (pdds_cat== 1,"Y","N")) 
Ensem_test_rf$pdds_cat = as.factor(Ensem_test_rf$pdds_cat)
Ensem_train_rf$pdds_cat = as.factor(Ensem_train_rf$pdds_cat)
# ----------------------------------
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random",classProbs = TRUE)
set.seed(2042)
mtry <- 2 
rf_random <- train(pdds_cat~., data=Ensem_train_rf, method="rf", metric="ROC", tuneLength=15, trControl=control)
```


```{r}
pred <- predict(rf, x_test, type="prob")[,2]
  new_pred_rf = as.data.frame(ifelse(pred >= cutoff, 1, 0))
  pred_train <- predict(rf, x, type="prob")[,2]
  new_pred_rf_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))
  data_rf = cbind(test[,c(outcome)], new_pred_rf)
  names(data_rf) = c("actual", "pred")
  xtab_rf = table(data_rf$actual, data_rf$pred)
    
  cm_rf = confusionMatrix(xtab_rf, mode = "everything", positive="1")
  cm_rf
  F1Score = round(as.data.frame(cm_rf$byClass[c(1,2:5,7)]),2)
  names(F1Score)= c("Coefficient")
  # Model Performance Statistics
  pred_val <-prediction(pred, y_test)
  auc = roc(as.factor(y_test)~ pred)
  ci = as.numeric(ci.auc(auc))
  auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
  names(auc1) = c("Coefficient")
  row.names(auc1) = c("Combined")
  F1Score[nrow(F1Score)+1,] = auc1
  # Calculating Area under Curve
  perf_val <- performance(pred_val,"auc")
  # Calculating True Positive and False Positive Rate
  perf_1 <- performance(pred_val, "tpr", "fpr")
  auc11 = round(perf_val@y.values[[1]],2)
```



## XGBoost
```{r}
cutoff = 0.4
y_train = as.character(unlist(Ensem_train[,(names(Ensem_train) %in% outcome)]))
combine = rbind(Ensem_test,Ensem_train)
xgb.train.data <- xgb.DMatrix(data = x, label = as.numeric(y_train),missing = NA)
param <- list(objective = "binary:logistic", base_score = 0.5)
all_x <- data.matrix(combine[,!(names(combine) %in% outcome)])
all_y = as.character(unlist(combine[,(names(combine) %in% outcome)]))
xgb.train.data2 <- xgb.DMatrix(data = all_x, label = as.numeric(all_y),missing = NA)

cv <- xgb.cv(data = xgb.train.data2,  nrounds = 1500, nthread = 20, nfold = 5, metrics = list("auc"),max_depth = 20,early_stopping_rounds = 100, eta = 1, objective = "binary:logistic")
bestIte = cv$best_iteration

best_model <- xgboost(param =param,  data = xgb.train.data, nrounds=bestIte)
pred <- predict(best_model, x_test, type="prob")
new_pred_xgb = as.data.frame(ifelse(pred >= cutoff, 1, 0))
data_xgb = cbind(Ensem_test[,c(outcome)], new_pred_xgb)
names(data_xgb) = c("actual", "pred")
xtab_xgb = table(data_xgb$actual, data_xgb$pred)
  
cm_xgb = confusionMatrix(xtab_xgb, mode = "everything", positive="1")
cm_xgb
F1Score = round(as.data.frame(cm_xgb$byClass[c(1,2:5,7)]),2)


# Model Performance Statistics
pred_val <-prediction(pred, y_test)
auc = roc(y_test~ pred)
ci = as.numeric(ci.auc(auc))

auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")
F1Score[nrow(F1Score)+1,] = auc1

auc13 = (round(ci[2],2))
```

## SVM
```{r}
seed = 2042
set.seed(seed)
model_svm = svm(pdds_cat ~ .,
               data = Ensem_train,
               cost = 10,scale = T,
               kernel = 'radial')
cutoff = 0.25
class1.svm.model <- svm(pdds_cat ~ ., data = Ensem_train,cost=0.1, cross=10,type="C-classification",kernel="radial",na.action=na.omit)

tmodel=tune(svm,pdds_cat ~., data=Ensem_train,
ranges=list(cost = c(0.001, 0.01, 0.1, 1,5,10,15)))
best_model = tmodel$best.model
pred <- predict(class1.svm.model, x_test, type="response")
# new_pred_svm = as.data.frame(ifelse(pred >= cutoff, 1, 0))  
data_svm = cbind(test[,c(outcome)], pred)
names(data_svm) = c("actual", "pred")
xtab_svm = table(data_svm$actual, data_svm$pred)
  

new_pred_svm <- predict(best_model, x_test, type="prob")
# new_pred_svm = as.data.frame(ifelse(pred >= cutoff, 1, 0))  

new_pred_svm_train <- predict(best_model, x, type="prob")
# new_pred_svm_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))

data_svm = cbind(test[,c(outcome)], new_pred_svm)
names(data_svm) = c("actual", "pred")
xtab_svm = table(data_svm$actual, data_svm$pred)
  
cm_svm = confusionMatrix(xtab_svm, mode = "everything", positive="1")
F1Score = round(as.data.frame(cm_svm$byClass[c(1,2:5,7)]),2)
names(F1Score)= c("Coefficient")

# Model Performance Statistics
pred_val <-prediction(as.numeric(new_pred_svm), y_test)
auc = roc(y_test~ as.numeric(new_pred_svm))
ci = as.numeric(ci.auc(auc))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")

cm_svm = confusionMatrix(xtab_svm, mode = "everything", positive="1")
F1Score = round(as.data.frame(cm_svm$byClass[c(1,2:5,7)]),2)
names(F1Score)= c("Coefficient")
auc = roc(y_test~ as.numeric(new_pred_svm))
ci = as.numeric(ci.auc(auc))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")
F1Score[nrow(F1Score)+1,] = auc1
auc44 = paste(round(ci[2],2))

```

## LASSO

```{r}
outcome = c("pdds_cat")
cutoff= 0.4

lambdas <- 10^seq(2, -3, length = 500)
cv_model <- cv.glmnet(x, as.numeric(as.character(y_train)), alpha = 1,lambda = lambdas, standardize = TRUE,nfolds = 10, type = "auc")

best_lambda <- cv_model$lambda.min
best_model <- glmnet(x, as.numeric(as.character(y_train)), alpha = 1, lambda = best_lambda)

predictions_test <- predict(best_model, s = best_lambda, newx = x_test)

new_pred_lasso = as.data.frame(ifelse(predictions_test >= cutoff, 1, 0))

data_lasso = cbind(Ensem_test[,c(outcome)], new_pred_lasso)
names(data_lasso) = c("actual", "pred")
xtab_lasso = table(data_lasso$actual, data_lasso$pred)

cm_lasso = confusionMatrix(xtab_lasso, mode = "everything", positive="1")
cm_lasso
F1Score = round(as.data.frame(cm_lasso$byClass[c(1,2:5,7)]),2)
names(F1Score)= c("Coefficient")
Ensem_test$lasso.prob <- predict(best_model,type="response",newx = x_test, s = 'best_lambda')
pred <- prediction(Ensem_test$lasso.prob, Ensem_test[, c(outcome)])
auc = roc(y_test~ predictions_test)
ci = as.numeric(ci.auc(auc))
auc12 = (round(ci[2],2))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")
F1Score[nrow(F1Score)+1,] = auc1
  
```

# PROMIS Ensemble 


```{r}
set.seed(23)
ratio = .8
outcome = c("PROMIS_Bi")
PROMIS_allpatients =  All_patients[complete.cases(All_patients),c("id",outcome)]
dt = (sample(nrow(PROMIS_allpatients), nrow(PROMIS_allpatients)*ratio))
train<-PROMIS_allpatients[dt,]
test<-PROMIS_allpatients[-dt,]

PROMIS_train = as.factor(train$PROMIS_Bi)
PROMIS_test =as.factor(test$PROMIS_Bi)
ensem_df = preds_xgb_PROMIS %>% left_join(preds_lasso_PROMIS , by = c("xgb_PROMIS_combine.preds.test_id" = "Lasso_Binary_coef_PROMIS.preds_train.train_id")) %>% 
left_join(pred_SVM_PROMIS,by = c("xgb_PROMIS_combine.preds.test_id" = "SVM_PROMIS_combine.preds_train.train_id"))  %>% 
  left_join(preds_rf_PROMIS, by = c("xgb_PROMIS_combine.preds.test_id" = "rf_PROMIS_combine.preds.test_id" )) %>% rename(id = xgb_PROMIS_combine.preds.test_id)


ensemble_names = c("xgb","lasso","svm","rf")
Ensem_train = train %>% left_join(ensem_df )
Ensem_test = test %>% left_join(ensem_df)
Ensem_train$PROMIS_Bi = as.factor(train$PROMIS_Bi)
pdds_test$PROMIS_Bi =as.factor(test$PROMIS_Bi)
Ensem_train = Ensem_train[,-1]
Ensem_test = Ensem_test[,-1]
ensemble_names = c("xgb","lasso","svm","rf")
names(Ensem_train) = c(outcome,ensemble_names)
names(Ensem_test) = c(outcome,ensemble_names)


y_train = as.factor(as.character(unlist(Ensem_train[,(names(Ensem_train) %in% outcome)])))
x <- data.matrix(Ensem_train[,!(names(Ensem_train) %in% outcome)])
y_test = as.numeric(as.character(unlist(Ensem_test[,(names(Ensem_test) %in% outcome)])))
x_test <- data.matrix(Ensem_test[,!(names(Ensem_test) %in% outcome)])
```

## XGBoost
```{r}
seed = 23

cutoff = 0.55
y_train = as.character(unlist(Ensem_train[,(names(Ensem_train) %in% outcome)]))
combine = rbind(Ensem_test,Ensem_train)
xgb.train.data <- xgb.DMatrix(data = x, label = as.numeric(y_train),missing = NA)
param <- list(objective = "binary:logistic", base_score = 0.5)
all_x <- data.matrix(combine[,!(names(combine) %in% outcome)])
all_y = as.character(unlist(combine[,(names(combine) %in% outcome)]))
xgb.train.data2 <- xgb.DMatrix(data = all_x, label = as.numeric(all_y),missing = NA)

cv <- xgb.cv(data = xgb.train.data2,  nrounds = 1500, nthread = 20, nfold = 5, metrics = list("auc"),max_depth = 20,early_stopping_rounds = 100, eta = 1, objective = "binary:logistic")
bestIte = cv$best_iteration

best_model <- xgboost(param =param,  data = xgb.train.data, nrounds=bestIte)
pred <- predict(best_model, x_test, type="prob")
new_pred_xgb = as.data.frame(ifelse(pred >= cutoff, 1, 0))
data_xgb = cbind(test[,c(outcome)], new_pred_xgb)
names(data_xgb) = c("actual", "pred")
xtab_xgb = table(data_xgb$actual, data_xgb$pred)
  
cm_xgb = confusionMatrix(xtab_xgb, mode = "everything", positive="1")
cm_xgb

  # Model Performance Statistics
pred_val <-prediction(pred, y_test)
F1Score = round(as.data.frame(cm_xgb$byClass[c(1,2:5,7)]),2)
auc = roc(y_test~ pred)
ci = as.numeric(ci.auc(auc))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")
F1Score[nrow(F1Score)+1,] = auc1
auc3pr = (round(ci[2],2))


```

## RF 
```{r}
rf <- randomForest( PROMIS_Bi ~ .,  data=Ensem_train,ntree = 1000)
    
cutoff = 0.6

pred <- predict(rf, x_test, type="prob")[,2]
new_pred_rf = as.data.frame(ifelse(pred >= cutoff, 1, 0))
pred_train <- predict(rf, x, type="prob")[,2]
new_pred_rf_train = as.data.frame(ifelse(pred_train >= cutoff, 1, 0))
data_rf = cbind(Ensem_test[,c(outcome)], new_pred_rf)
names(data_rf) = c("actual", "pred")
xtab_rf = table(data_rf$actual, data_rf$pred)
  
cm_rf = confusionMatrix(xtab_rf, mode = "everything", positive="1")
cm_rf

F1Score = round(as.data.frame(cm_rf$byClass[c(1,2:5,7)]),2)
names(F1Score)= c("Coefficient")
# Model Performance Statistics
pred_val <-prediction(pred, y_test)
auc = roc(y_test~ pred)
ci = as.numeric(ci.auc(auc))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")
F1Score[nrow(F1Score)+1,] = auc1

perf_val <- performance(pred_val,"auc")
auc1pr = round(perf_val@y.values[[1]],2)

```
## LASSO

```{r}
seed = 23
ratio = 0.6
run_comb = F
outcome = c("PROMIS_Bi")
cutoff= .6

lambdas <- 10^seq(2, -3, length = 500)
cv_model <- cv.glmnet(x, as.numeric(as.character(y_train)), alpha = 1,lambda = lambdas, standardize = TRUE,nfolds = 10, type = "auc")

best_lambda <- cv_model$lambda.min
best_model <- glmnet(x, as.numeric(as.character(y_train)), alpha = 1, lambda = best_lambda)

predictions_test <- predict(best_model, s = best_lambda, newx = x_test)

new_pred_lasso = as.data.frame(ifelse(predictions_test >= cutoff, 1, 0))

data_lasso = cbind(test[,c(outcome)], new_pred_lasso)
names(data_lasso) = c("actual", "pred")
xtab_lasso = table(data_lasso$actual, data_lasso$pred)

cm_lasso = confusionMatrix(xtab_lasso, mode = "everything", positive="1")
cm_lasso
F1Score = round(as.data.frame(cm_lasso$byClass[c(1,2:5,7)]),2)
names(F1Score)= c("Coefficient")

Ensem_test$lasso.prob <- predict(best_model,type="response",newx = x_test, s = 'best_lambda')
pred <- prediction(Ensem_test$lasso.prob, Ensem_test[, c(outcome)])
auc = roc(y_test~ predictions_test)
ci = as.numeric(ci.auc(auc))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")
F1Score[nrow(F1Score)+1,] = auc1
auc2 = (round(ci[2],2))

```


## SVM
```{r}
seed = 23

set.seed(seed)
model_svm = svm(PROMIS_Bi ~ .,
               data = Ensem_train,
               cost = 10,scale = T,
               kernel = 'radial')



  tmodel=tune(svm,PROMIS_Bi ~., data=Ensem_train,
ranges=list(cost = c(0.001, 0.01, 0.1, 1,5,10,15)))
best_model = tmodel$best.model
new_pred_svm <- predict(best_model, x_test, type="prob")

new_pred_svm_train <- predict(best_model, x, type="prob")
data_svm = cbind(test[,c(outcome)], new_pred_svm)
names(data_svm) = c("actual", "pred")
xtab_svm = table(data_svm$actual, data_svm$pred)
  
cm_svm = confusionMatrix(xtab_svm, mode = "everything", positive="1")
F1Score = round(as.data.frame(cm_svm$byClass[c(1,2:5,7)]),2)
names(F1Score)= c("Coefficient")

# Model Performance Statistics
pred_val <-prediction(as.numeric(new_pred_svm), y_test)
auc = roc(y_test~ as.numeric(new_pred_svm))
ci = as.numeric(ci.auc(auc))
auc1 = as.data.frame(c(paste(round(ci[2],2),"(",round(ci[1],2),",",round(ci[3],2),")")))
names(auc1) = c("Coefficient")
row.names(auc1) = c("Combined")

F1Score[nrow(F1Score)+1,] = auc1
auc4Promis = paste(round(ci[2],2))

  
```

